{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cecd2b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up the path\n",
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.abspath(\"..\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "946cc572",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing necessary libraries\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.document_loaders import UnstructuredHTMLLoader\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.llms import HuggingFacePipeline\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "\n",
    "import pyttsx3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b049e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "loader = UnstructuredHTMLLoader(file_path=\"../data/mg-zs-warning-messages.html\")\n",
    "car_docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9f12dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=400, chunk_overlap=50)\n",
    "splits = text_splitter.split_documents(car_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2816fbd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\AllProjects\\2026\\Building-RAG-Chatbots-for-Technical-Documentation\\venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# loaclly create embeddings and vectorstore\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "vectorstore = Chroma.from_documents(documents=splits,embedding=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17c6c267",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup retriever\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4ecb1f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\AllProjects\\2026\\Building-RAG-Chatbots-for-Technical-Documentation\\venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "C:\\Users\\mohid\\AppData\\Local\\Temp\\ipykernel_9760\\4042260309.py:14: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n",
      "  llm = HuggingFacePipeline(pipeline=pipe)\n"
     ]
    }
   ],
   "source": [
    "# local LLM setup, runs on CPU, can be changed to a smaller model to run on a laptop\n",
    "model_name = \"google/flan-t5-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=150,\n",
    "    do_sample=False\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2d8222c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG prompt template\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"You are an assistant for question-answering tasks.\n",
    "Use the following context to answer the question.\n",
    "If you don't know the answer, say you don't know.\n",
    "\n",
    "Question: {question}\n",
    "Context: {context}\n",
    "\n",
    "Answer:\"\"\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52f82449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build offline RAG chain\n",
    "rag_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4f46c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# question\n",
    "question = \"What does the brake system warning mean?\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5d0c7b6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:\n",
      "Brake Fault Indicates that the brake fluid could be low or a fault has been detected in the Electronic Brake-force Distribution (EBD) system.\n"
     ]
    }
   ],
   "source": [
    "# ask question\n",
    "response = rag_chain.invoke(question)\n",
    "\n",
    "# extract text safely\n",
    "answer_text = response.content if hasattr(response, \"content\") else str(response)\n",
    "\n",
    "# print text answer\n",
    "print(f\"Answer:\\n{answer_text}.\")\n",
    "\n",
    "# initialize TTS engine\n",
    "engine = pyttsx3.init()\n",
    "\n",
    "# adjust voice speed (default ~200)\n",
    "engine.setProperty(\"rate\", 200)\n",
    "\n",
    "# adjust volume (0.0 to 1.0)\n",
    "engine.setProperty(\"volume\", 1.0)\n",
    "\n",
    "# set 0 or 1 for different voices\n",
    "voices = engine.getProperty('voices')\n",
    "engine.setProperty('voice', voices[1].id)\n",
    "\n",
    "# speak immediately\n",
    "engine.say(answer_text)\n",
    "engine.runAndWait()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335d80f1",
   "metadata": {},
   "source": [
    "#### **Note:** Prototype is done, implement further requirements, and build app for deployment."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Welcome to DataCamp Workspaces.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
